#!/bin/bash
#############################################################
##
## Dug is semantic search framework for digging in dark data
##
##   Crawl & Index:
##
##     crawl: Execute graph queries against an aggregator.
##            Record the knowledge graphs in a cache.
##       usage: bin/dug crawl
##
##   Search API:
##
##     api: Provides a REST API to the search engine.
##       bin/dug api [--debug] [--port=<int>]
##
##   Development:
##
##     stack: Run search engine, neo4j, redis, and the
##            search OpenAPI endpoint.
##
##       usage: bin/dug stack [service ]*
##
##     dev init: Run once before any services to generate
##         docker/.env containing passwords, etc.
##       usage: bin/dug dev init
##
##     dev conf: Is run automatically in this script to
##         source docker/.env and make env variables
##         available to all client applications.
##       usage: bin/dug dev conf
##
##     test: Run automated functional tests.
##       usage: bin/dug test
##
#############################################################

# Configure the PYTHONPATH
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
DUG_HOME=$( dirname $DIR )
DUGENV=$DUG_HOME/docker/.env
export PYTHONPATH=$DUG_HOME

# for debugging
set -x

# Development environment.
dev () {
    init () {
    	if [[ ! -f $DUGENV ]]; then
            # This must be run once and only once before running the stack.
            # It generates a docker-compose environment file, including
            # passwords for the services. This is used when executing docker-compose.
	    echo "generate .env file..."
	    HOSTNAME=$HOSTNAME RANDOM=$RANDOM envsubst < $DUG_HOME/docker/.env.template > $DUGENV

            # The .env.template used above specifies service hostnames consistent with the ones
            # used inside the docker-compose specification. Here, we create a parallel environment
            # file with hostnames all set to localhost but keeping other settings the same. This allows
            # tools we run in development to connect to services running in compose.
	    echo "generate .env.dev file..."
            cat $DUGENV | sed -e "s,neo4j,localhost,g" \
                              -e "s,elasticsearch,localhost,g" \
                              -e "s,redis,localhost,g" > $DUGENV.dev
	fi
        source $DUGENV.dev
        export $(cut -d= -f1 $DUGENV.dev)
    }
    $*
}

#############################################################
##
## Crawl: Gather knowledge graphs from TranQL, organize by tag, create indices, and add to search engine.
##
#############################################################
crawl () {
    python -m dug.core $*
}


#############################################################
##
## Crawl_by_dir: do a crawl_by_concept on all csv and xml files in a directory one at a time
##
#############################################################
crawl_dir () {
    input_dir=$1
    parser_type=$2
    element_type=$3
    
    # set element_type to parser_type if not set
    if [ -z "$element_type"]; then
        element_type=$parser_type
    fi
    
    # Report how many files are in dir
    echo "Crawling directory: $1"
    init_files=$(find $input_dir \( -name '*.xml' -o -name '*.csv' \) -type 'f' | wc -l)
    echo "Found $init_files files to crawl"

    # Loop through all files that match pattern, do crawl by concept, and then delete file if finished successfully
    find $input_dir \( -name '*.xml' -o -name '*.csv' \) -type 'f' -exec python -m dug.core --crawl-file {} --parser-type $parser_type --element-type $element_type \; -exec rm {} \;

    # Count total number of files left that match pattern
    final_files=$(find $input_dir \( -name '*.xml' -o -name '*.csv' \) -type 'f' | wc -l)
    completed_files=$(( $init_files - $final_files ))
    echo "Successfully completed $completed_files crawls"
    echo "$(( $init_files - $completed_files)) files could not be crawled"
}

#############################################################
##
## Stack: Start the system's essential services.
##
#############################################################
stack () {
    source $DUGENV
    export $(cut -d= -f1 $DUGENV)

    if [[ ! -f $REDIS_DATA/appendonly.aof  ]]; then
      echo "Initializing redis with baseline AOF..."
      cp $DUG_HOME/data/redis/appendonly.aof $REDIS_DATA/
    fi

    if [[ ! -d $DUG_HOME/dug-search-client ]]; then
      echo "Initializing DUG search client..."
      git clone --single-branch --branch develop https://github.com/helxplatform/dug-search-client.git $DUG_HOME/dug-search-client
      cat <<- EOF > $DUG_HOME/dug-search-client/.env
REACT_APP_DUG_URL=http://127.0.0.1:5551
CLIENT_PORT=3030
PUBLIC_URL=/ui
EOF
    fi

    CLIENT_PORT=80 docker-compose -f $DUG_HOME/dug-search-client/docker-compose-dev.yaml $* &

    HOSTNAME=$HOSTNAME docker-compose \
	    --env-file $DUG_HOME/docker/.env \
	    -f $DUG_HOME/docker/docker-compose.yaml -p dug $*
}

#############################################################
##
## Python Tests: Run the python unit tests for DUG
##
#############################################################
pytests () {
  set -e

  # Create the virtualenv if it doesn't exist already and
  # install the required modules to the venv
  if [[ ! -d $DUG_HOME/venv ]]; then
    # TODO: Check that python3 exists before trying to use it
    python3 -m venv $DUG_HOME/venv
    source $DUG_HOME/venv/bin/activate
    pip install -r $DUG_HOME/requirements.txt
  fi

  # NOTE: In certain scenarios we'll run the venv/bin/activate
  # script twice, which is okay, since it's idempotent
  source $DUG_HOME/venv/bin/activate
  cd $DUG_HOME/dug/tests && python3 -m pytest
  deactivate && cd $DUG_HOME

  set +e
}

#############################################################
##
## Query_API: Query the index via the search API.
##
#############################################################
query_api () {
     query="`echo '{"index" : "concepts_index", "query" : "'$*'"}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search
}

#############################################################
##
## Query_NBoost: Query the index via the search API, but
##               using nboost.
##
#############################################################
query_nboost () {
     query="`echo '{"index" : "test", "query" : "'$*'", "boosted": 1}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search
}

#############################################################
##
## Query_Var_API: Query the element/variable index via the search API.
##
#############################################################
query_var_api () {
     concept_id=$1
     size=$2
     data_type=$3
     query="`echo '{"index" : "variables_index", "query" : "'${@:4}'", "concept" : "'$concept_id'", "data_type" : "'$data_type'", "size": '$size'}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search_var
}

#############################################################
##
## Data_Types_API: Return the aggregate data types of all elements.
##
#############################################################
query_datatypes_api () {
     curl --data '{"index": "variables_index"}' \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/agg_data_types
}

#############################################################
##
## Query_KG_API: Query the knowledge-graph index via the search API.
##
#############################################################
query_kg_api () {
     unique_id=$1
     query="`echo '{"index" : "kg_index", "query" : "'${@:2}'", "unique_id" : "'$unique_id'"}'`"
     curl --data "$query" \
           --header "Content-Type: application/json" \
           --request POST \
           http://localhost:5551/search_kg
}

dev init

$*

exit 0
